# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree. In this project I built **Azure ML pipeline** using **Python SDK** and a custom **Scikit-learn Logistic Regression** model. Hyperparameters of the model were optimized using Hyperdrive. After this Azure AutoML was used to find optimal model using the same dataset, and the **results of the two methods were compared**. 

See the diagram below showing the main steps followed in the project:

![alt text](images/Project_process_overview.png)

**Step 1:** Setting up the training [script](train.py), to create a Tabular Dataset from imported file, clean and split the data for Scikit-learn logistic regression model. 

**Step 2:** Creating [Jupyter Notebook](udacity-project.ipynb) and configuring HyperDrive to find the best hyperparameters for the logistic regression model. 

**Step 3:** Loading the same dataset, as used with Scikit-learn model, with TabularDatasetFactory and using AutoML to find an optimized model. 

**Step 4:** Comparing the results of the two methods and writing a research summary, this RAEADME.md.

## Summary
The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y). 

The best performing model with **0.91627 accuracy** was the **AutoML model** which used **VotingEnsemble** algorith. The best **HyperDrive model** using **Scikit-learn logistic regression** had nearly as good accuracy of **0.90895**.

## Scikit-learn Pipeline

Built Scikit-learn pipeline uses [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model for classification, while the hyperparameters were tuned using [HyperDrive](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters). The used hyperparameters were C (Inverse of regularization strength) with unform value distribution and max_iter (Maximum number of iterations taken for the solvers to converge) with discrete values for choice. 

**Parameter Sampling**

RandomParameterSampling, where hyperparameter values are randomly selected from the defined search space, was used as a sampler. It is a good choice as it is [more efficient, though less exhaustive compared](https://www.sciencedirect.com/science/article/pii/S1674862X19300047) to Grid grid search to search over the search space.

```
ps = RandomParameterSampling({
    '--max_iter' : choice(20,40,80,100,150,200),
    '--C' : uniform(0.001,10)
}) 
```
**Early Stopping Policy**

Early stopping policy was used to terminate poorly performing runs, this also improves computational efficiency. Here [**Bandit policy**](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters) was used as an early stopping policy, and was configured as follows:
```
policy = BanditPolicy(evaluation_interval=2,slack_factor=0.1,delay_evaluation=1)
```
In this example, the early termination policy is applied at every second interval when metrics are reported, starting at evaluation interval 1. Any run whose best metric is less than (1/(1+0.1) or 91% of the best performing run will be terminated.


**What are the benefits of the early stopping policy you chose?**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
